{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Icjb2J_jncQG","oRs2Jk7dT61D","_lA_X6ABfprL","V4Nbg-h1nWbS","J9QC1d4l8-gs"],"mount_file_id":"10hvAEFndfQz4CJ_jB7O5Ac8qVpBVGd-P","authorship_tag":"ABX9TyMEFER/od5nfv43he9LSnxk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sjyXtqTX4OFm"},"outputs":[],"source":["from bs4 import BeautifulSoup # for parsing and extracting value from requests's response\n","import requests # to make a HTTP request to certain URL\n","import pandas as pd # for data processing\n","import numpy as np # for math operations\n","import time # for timing the operations later in scrapping\n","import random # for generating random number\n","import ast # to convert value from string to dictionary or lists\n","\n","pd.set_option('display.max_colwidth',None) # set display for pandas columns width"]},{"cell_type":"markdown","source":["# Scrape Phone Specs from gsmarena"],"metadata":{"id":"Icjb2J_jncQG"}},{"cell_type":"markdown","source":["2 April 2024"],"metadata":{"id":"X-r9J77Ior-D"}},{"cell_type":"code","source":["# Create URL variable so that can easily be called later\n","# Don't forget to create headers IMPORTANT!\n","\n","base_url = 'https://www.gsmarena.id/'\n","headers = {\n","'Accept':'*/*',\n","'Accept-Language':'id,en;q=0.9,en-GB;q=0.8,en-US;q=0.7',\n","'Cookie':'AEC=Ae3NU9OiuWVQT7dFhyPElqEoRacGPjT65AopViuKChN9IDBllqDIJWe2pg; SID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7NY_7r-0bGDDVPK4Op3EIUgACgYKAfQSAQASFQHGX2MiqWjJQzpi0ukbqFk3p6-2ExoVAUF8yKqHCCTXIWoD6f_8gWvSUxWj0076; __Secure-1PSID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7Q02tSpksVkeFYbcPRZrwTwACgYKAVISAQASFQHGX2MiSVZXNDDEfWEZ5r3WRRdP4BoVAUF8yKqhCD_lLOnBvRGvKfBcvgm70076; __Secure-3PSID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7otvtAXW86ow_62OFn-dpBQACgYKAWcSAQASFQHGX2Mij-qF2EMjSA3x6SzHWtaBTBoVAUF8yKpDUO5stx2B1FxwV018uMM00076; HSID=AnoiWEq-xDj2N3_3P; SSID=AYWMluqrHQKjwI-Lh; APISID=bsyQaFNzZhgplnlQ/A-ld160wSkvVjoidL; SAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; __Secure-1PAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; __Secure-3PAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; NID=513=BzDD4Yil-JG-Kyb__D_8deueF9zUvFhX1FGZMSMY-R6k3Ar0ihZnp8nAN3LalhifChEjBHQSaKF5oSMIGHwv4MlJ9OxCNGI76CXJ2TgCA559NI8-yUVPAsxspCbBY9bjurpJWlqWfYfz_45SJkg9rbiClaItAzEDXhnhmOdVH-8eOAimDS2-r0XyDkTyKsAkUGLwofWe6JMpSybXAmt1D-4mDSFxeB32SJVLLPfHSnp_jr8sieTirvxfK80J7ptJ4xy0TT2ubFiTwohZmuSwIsaCGojOwWRmH5kYH0CQcvxVqIKlpiAIxKUmj5fWZOMqi94Eu3t7GPxBa1g; __Secure-1PSIDTS=sidts-CjIB7F1E_Hyt5KG9lXCtn6pZe6S1AmMlj-Q4gWWNjaEslgZo7D709YjYOoHislHWwc9bQRAA; __Secure-3PSIDTS=sidts-CjIB7F1E_Hyt5KG9lXCtn6pZe6S1AmMlj-Q4gWWNjaEslgZo7D709YjYOoHislHWwc9bQRAA; OGPC=19010599-1:19011583-1:; OGP=-19010599:-19011583:; DV=o9yiE6fVZS9d0KXi2U5DAWIhLUwe6thjFs55AWIZeAAAAEBL1W1MikEpVgAAAOBRX6N8aZALRQAAALwkxYOqKaGkFwAAAA; UULE=a+cm9sZTogMQpwcm9kdWNlcjogMTIKdGltZXN0YW1wOiAxNzEyMTEzMTE1ODEwMDAwCmxhdGxuZyB7CiAgbGF0aXR1ZGVfZTc6IC02OTcxNDAwMAogIGxvbmdpdHVkZV9lNzogMTEwNDI1NDAwMAp9CnJhZGl1czogNDUxOTU2NDQwCnByb3ZlbmFuY2U6IDYK; SIDCC=AKEyXzXSlJOTX8psC94ZNMwZmnN3YWjyaDr5OR2VkyD6jCmyxZSkbafuhD8yTG83p-MRPkPsIXU; __Secure-1PSIDCC=AKEyXzX5d2p9wKHT6IyXTZc4wCRl8t8RDfpcFW4nw8toleGLkDcL_41cvooYJIxqnAwkX9hq3JY; __Secure-3PSIDCC=AKEyXzWgFvoEWKW_rS4FWY3lUi_ray42A7kWEY8HWeXjL9hEBUeD2-TR24xlw4JDOVJs5PguuMI',\n","'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0',\n","'Cache-Control': 'max-age=0',\n","'Connection': 'keep-alive',\n","'Upgrade-Insecure-Requests': '1',\n","}"],"metadata":{"id":"jBm8nVoH4oG6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start a requests to URL variable and create variable content\n","res = requests.get(base_url,headers=headers)\n","html = res.content\n","\n","# Parsed the content using BeautifulSoup\n","htmlParsed = BeautifulSoup(html,'html.parser')\n","merk = htmlParsed.find('ul',attrs={\"class\": \"columns\"})\n","\n","# Looping through the parsed html to find all the anchor elements ('a') and insert it to empty list\n","link_merk = []\n","for a in merk.find_all('a'):\n","  link_merk.append(a['href'])"],"metadata":{"id":"tt2g8ggF5BT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Empty list to catch the content from looping later\n","products = []\n","link_prods = []\n","\n","# Looping through the list of link_merk created earlier and repeat the same processes\n","for url in link_merk:\n","  # Start a requests to URL variable and create variable content\n","  resp = requests.get(url)\n","  htmldata = resp.content\n","  # Parsed the content using BeautifulSoup\n","  parsed = BeautifulSoup(htmldata,'html.parser')\n","  prod = parsed.find('ul',attrs={\"class\":\"vs-ki\"})\n","  # Set the sleep time so it doesn't look much like robot\n","  sleepTime = random.uniform(1, 10)\n","  time.sleep(sleepTime)\n","  # Same process\n","  product = []\n","  link_prod = []\n","  # Nested loop to find certain elements\n","  for a in prod.find_all('a'):\n","      product.append(a['title'])\n","      link_prod.append(a['href'])\n","  # Insert values / list of product & link product to first 2 empty list on top\n","  products.append(product)\n","  link_prods.append(link_prod)"],"metadata":{"id":"VWQ5mpWE6-zk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the products & link prods list\n","all_prod = []\n","all_link = []\n","for a in products:\n","  for b in a:\n","    all_prod.append(b)\n","for a in link_prods:\n","  for b in a:\n","    all_link.append(b)"],"metadata":{"id":"-qQUqbim7ZAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataframe using pandas from to lists above\n","df = pd.DataFrame()\n","df['products'] = pd.Series(all_prod)\n","df['linkProducts'] = pd.Series(all_link)"],"metadata":{"id":"RcpC98T_9xHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create URL for another Looping later\n","base = 'https://www.gsmarena.id'\n","df['linkProducts'] = base+df['linkProducts']"],"metadata":{"id":"sKu1bgNZvWpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["specs = []\n","# Looping through the products link to extract the specifications info\n","for link in df['linkProducts']:\n","  # Same process\n","  response = requests.get(link,headers=headers,allow_redirects=False)\n","\n","  htmlData = response.content\n","  parsedHtml = BeautifulSoup(htmlData,'html.parser')\n","\n","  # Find a table element from the content\n","  table = parsedHtml.find('tbody')\n","  time.sleep(sleepTime)\n","  # Empty list to catch values from the scraped table\n","  spek = []\n","  # Extract all values from the table\n","  for x in table.find_all('td'):\n","    spek.append(x)\n","\n","  # Create a dictionary or key, value pair from the spek list based on nth spek element\n","  #  if the nth element is odd than it is key, if the nth element is even then it is value\n","  key = []\n","  val = []\n","  for x in range(len(spek)):\n","    if x % 2 == 0:\n","      key.append(spek[x])\n","    else:\n","      val.append(spek[x])\n","\n","  spesifikasi = {k.text:v.text for k,v in zip(key,val)}\n","  # Insert those dictionaries into specs list\n","  specs.append(spesifikasi)"],"metadata":{"id":"mjpfy4SX_P4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add new column to the dataframe which contains specs list\n","df['specifications'] = pd.Series(specs)"],"metadata":{"id":"Zong3YvmALUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Empty list to catch the values from flatten dictionaries\n","announce = []\n","launch = []\n","price = []\n","network = []\n","dimension = []\n","weight = []\n","battery = []\n","screen = []\n","resolution = []\n","storage = []\n","os = []\n","chipset = []\n","cpu = []\n","gpu = []\n","maincam = []\n","frontcam = []\n","nfc = []\n","usb = []"],"metadata":{"id":"ou_w0BwZEEm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Looping through the dictionaries to insert the value into certain list\n","for x in df['specifications'].values:\n","  # Using try & exception to handle error, if certain key doesn't exist within dictionary it will insert empty string\n","  try:\n","    announce.append(x['Diumumkan'])\n","  except:\n","    announce.append('')\n","  try:\n","    launch.append(x['Status'])\n","  except:\n","    launch.append('')\n","  try:\n","    price.append(x['Expected Price'])\n","  except:\n","    price.append('')\n","  try:\n","    network.append(x['Teknologi'])\n","  except:\n","    network.append('')\n","  try:\n","    dimension.append(x['Dimensi'])\n","  except:\n","    dimension.append('')\n","  try:\n","    weight.append(x['Berat'])\n","  except:\n","    weight.append('')\n","  try:\n","    battery.append(x['Tipe'])\n","  except:\n","    battery.append('')\n","  try:\n","    screen.append(x['Ukuran'])\n","  except:\n","    screen.append('')\n","  try:\n","    resolution.append(x['Resolusi'])\n","  except:\n","    resolution.append('')\n","  try:\n","    storage.append(x['Internal'])\n","  except:\n","    storage.append('')\n","  try:\n","    os.append(x['Os'])\n","  except:\n","    os.append('')\n","  try:\n","    chipset.append(x['Chipset'])\n","  except:\n","    chipset.append('')\n","  try:\n","    cpu.append(x['Cpu'])\n","  except:\n","    cpu.append('')\n","  try:\n","    gpu.append(x['Gpu'])\n","  except:\n","    gpu.append('')\n","  try:\n","    maincam.append(x['Dual'])\n","  except:\n","    maincam.append('')\n","  try:\n","    frontcam.append(x['Single'])\n","  except:\n","    frontcam.append('')\n","  try:\n","    nfc.append(x['Nfc'])\n","  except:\n","    nfc.append('')\n","  try:\n","    usb.append(x['Usb'])\n","  except:\n","    usb.append('')"],"metadata":{"id":"u9uyhKCdMGtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create columns from the list above\n","df['annouced'] = pd.Series(announce)\n","df['launch'] = pd.Series(launch)\n","df['price'] = pd.Series(price)\n","df['network'] = pd.Series(network)\n","df['dimension'] = pd.Series(dimension)\n","df['weight'] = pd.Series(weight)\n","df['battery'] = pd.Series(battery)\n","df['screen'] = pd.Series(screen)\n","df['resolution'] = pd.Series(resolution)\n","df['storage'] = pd.Series(storage)\n","df['os'] = pd.Series(os)\n","df['chipset'] = pd.Series(chipset)\n","df['cpu'] = pd.Series(cpu)\n","df['gpu'] = pd.Series(gpu)\n","df['maincam'] = pd.Series(maincam)\n","df['frontcam'] = pd.Series(frontcam)\n","df['nfc'] = pd.Series(nfc)\n","df['usb'] = pd.Series(usb)"],"metadata":{"id":"z_v_LbgxO3oL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save it to CSV\n","\n","# uncomment & run line of code below to save the data into csv\n","# df.to_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - Smartphone/ScrappedRAW_Data/gsmarena.csv',index=False)"],"metadata":{"id":"wajbcjXPhj_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scrape Phone Specs from carisinyal"],"metadata":{"id":"oRs2Jk7dT61D"}},{"cell_type":"markdown","source":["23 April 2024"],"metadata":{"id":"J1_HHrB7oyY2"}},{"cell_type":"code","source":["# Create URL variable so that can easily be called later\n","# Don't forget to create headers IMPORTANT!\n","\n","base_url = 'https://carisinyal.com/hp/'\n","headers = {\n","'Accept':'*/*',\n","'Accept-Language':'id,en;q=0.9,en-GB;q=0.8,en-US;q=0.7',\n","'Cookie':'AEC=Ae3NU9OiuWVQT7dFhyPElqEoRacGPjT65AopViuKChN9IDBllqDIJWe2pg; SID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7NY_7r-0bGDDVPK4Op3EIUgACgYKAfQSAQASFQHGX2MiqWjJQzpi0ukbqFk3p6-2ExoVAUF8yKqHCCTXIWoD6f_8gWvSUxWj0076; __Secure-1PSID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7Q02tSpksVkeFYbcPRZrwTwACgYKAVISAQASFQHGX2MiSVZXNDDEfWEZ5r3WRRdP4BoVAUF8yKqhCD_lLOnBvRGvKfBcvgm70076; __Secure-3PSID=g.a000iAjO9NmaMCjKDHQkfW91b7ph0BpPfW9NM8iI6P2jmHLCrGX7otvtAXW86ow_62OFn-dpBQACgYKAWcSAQASFQHGX2Mij-qF2EMjSA3x6SzHWtaBTBoVAUF8yKpDUO5stx2B1FxwV018uMM00076; HSID=AnoiWEq-xDj2N3_3P; SSID=AYWMluqrHQKjwI-Lh; APISID=bsyQaFNzZhgplnlQ/A-ld160wSkvVjoidL; SAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; __Secure-1PAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; __Secure-3PAPISID=35O6u81qYSx9v_45/ApLmJ6jcg9jbcbZyr; NID=513=BzDD4Yil-JG-Kyb__D_8deueF9zUvFhX1FGZMSMY-R6k3Ar0ihZnp8nAN3LalhifChEjBHQSaKF5oSMIGHwv4MlJ9OxCNGI76CXJ2TgCA559NI8-yUVPAsxspCbBY9bjurpJWlqWfYfz_45SJkg9rbiClaItAzEDXhnhmOdVH-8eOAimDS2-r0XyDkTyKsAkUGLwofWe6JMpSybXAmt1D-4mDSFxeB32SJVLLPfHSnp_jr8sieTirvxfK80J7ptJ4xy0TT2ubFiTwohZmuSwIsaCGojOwWRmH5kYH0CQcvxVqIKlpiAIxKUmj5fWZOMqi94Eu3t7GPxBa1g; __Secure-1PSIDTS=sidts-CjIB7F1E_Hyt5KG9lXCtn6pZe6S1AmMlj-Q4gWWNjaEslgZo7D709YjYOoHislHWwc9bQRAA; __Secure-3PSIDTS=sidts-CjIB7F1E_Hyt5KG9lXCtn6pZe6S1AmMlj-Q4gWWNjaEslgZo7D709YjYOoHislHWwc9bQRAA; OGPC=19010599-1:19011583-1:; OGP=-19010599:-19011583:; DV=o9yiE6fVZS9d0KXi2U5DAWIhLUwe6thjFs55AWIZeAAAAEBL1W1MikEpVgAAAOBRX6N8aZALRQAAALwkxYOqKaGkFwAAAA; UULE=a+cm9sZTogMQpwcm9kdWNlcjogMTIKdGltZXN0YW1wOiAxNzEyMTEzMTE1ODEwMDAwCmxhdGxuZyB7CiAgbGF0aXR1ZGVfZTc6IC02OTcxNDAwMAogIGxvbmdpdHVkZV9lNzogMTEwNDI1NDAwMAp9CnJhZGl1czogNDUxOTU2NDQwCnByb3ZlbmFuY2U6IDYK; SIDCC=AKEyXzXSlJOTX8psC94ZNMwZmnN3YWjyaDr5OR2VkyD6jCmyxZSkbafuhD8yTG83p-MRPkPsIXU; __Secure-1PSIDCC=AKEyXzX5d2p9wKHT6IyXTZc4wCRl8t8RDfpcFW4nw8toleGLkDcL_41cvooYJIxqnAwkX9hq3JY; __Secure-3PSIDCC=AKEyXzWgFvoEWKW_rS4FWY3lUi_ray42A7kWEY8HWeXjL9hEBUeD2-TR24xlw4JDOVJs5PguuMI',\n","'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0',\n","'Cache-Control': 'max-age=0',\n","'Connection': 'keep-alive',\n","'Upgrade-Insecure-Requests': '1',\n","}"],"metadata":{"id":"Y_tEmx8ZT61S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start a requests to URL variable and create variable content\n","res = requests.get(base_url,headers=headers)\n","html = res.content\n","\n","# Parsed the content using BeautifulSoup\n","htmlParsed = BeautifulSoup(html,'html.parser')\n","merk = htmlParsed.find('div',attrs={\"class\": \"cities\"})\n","\n","# Looping through the parsed html to find all the anchor elements ('a') and insert it to empty dictionary\n","link_merk = {}\n","for a in merk.find_all('a'):\n","  link_merk[a.text] = a['href'] # this will insert the brand name and the link into the empty dictionary"],"metadata":{"id":"cr56AN2mT61S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["link = {} # create a new dictionary to catch the link for each brand's product\n","for merk in link_merk.keys():\n","  r = requests.get(link_merk[merk])\n","  h = r.content\n","\n","  soup = BeautifulSoup(h,'html.parser')\n","  page = soup.find_all('a',attrs={\"class\": \"page-numbers\"}) # find the pagination using class attribute\n","  try:\n","    max_page = int(page[-2].text) # take the second last value from pagination and convert it into an integer\n","  except:\n","    max_page = 0 # if there are no pagination then assign it to 0\n","  products = soup.find_all('a',attrs={\"class\": \"oxy-post-title\"}) # this to take all the product card\n","  for a in products:\n","    link[a.text] = a['href'] # this will insert the name of products and the link into the empty link dictionary\n","  if max_page != 0: # check if there is a pagination or not\n","    for i in range(2,max_page+1): # if there is pagination then catch the number start from 2 to the last number in pagination\n","      url = '{}page/{}/'.format(link_merk[merk],i) # this will take the brand's link and concatenate it with page number\n","      rs = requests.get(url)\n","      ht = rs.content\n","      s = BeautifulSoup(ht,'html.parser')\n","      p = s.find_all('a',attrs={\"class\": \"oxy-post-title\"})\n","      for l in p:\n","        link.update({l.text: l['href']})"],"metadata":{"id":"-J85C1Vo9uJ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(link) # check the number of links have been scrapped"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7NwT_NQ9BXU","executionInfo":{"status":"ok","timestamp":1713876353224,"user_tz":-420,"elapsed":513,"user":{"displayName":"Alfian Khofi","userId":"00807384603454567370"}},"outputId":"650754e0-bf24-405c-de67-0fe0a8014b83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1951"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# create dataframe from the dictionary\n","prods = pd.DataFrame()\n","prods['products'] = link.keys()\n","prods['link'] = link.values()"],"metadata":{"id":"BIUAlQrJACO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save it into csv file just in case\n","path = '/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - Smartphone/ScrappedRAW_Data/'\n","prods.to_csv(f'{path}carisinyal.csv',index=False)"],"metadata":{"id":"AX9VLz0Dyuyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read the csv file using pandas read_csv\n","prods = pd.read_csv(f'{path}carisinyal.csv')"],"metadata":{"id":"bbHCABo3OBg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["specs = [] # create an empty list to save the values from looping\n","for y in prods['link']:\n","  response = requests.get(y,headers=headers,allow_redirects=False)\n","\n","  htmlData = response.content\n","  parsedHtml = BeautifulSoup(htmlData,'html.parser')\n","\n","  price = {}\n","  prc = parsedHtml.find('div',attrs={\"class\":\"ct-code-block\"}) # this will try to select element that contains price\n","\n","  spek = []\n","  for x in parsedHtml.find_all('td'): # this to loop through the specifications\n","    spek.append(x)\n","\n","  # Create lists of key, value pair from the spek list based on nth spek element\n","  #  if the nth element is odd than it is key, if the nth element is even then it is value\n","  key = ['Harga']\n","  val = [prc.text]\n","\n","  for x in range(len(spek)):\n","    if x % 2 == 0:\n","      key.append(spek[x].text.strip())\n","    else:\n","      val.append(spek[x].text.strip())\n","\n","  price.update({'key':key,'val':val}) # create dictionary from two lists above\n","\n","  # Insert those dictionaries into specs list\n","  specs.append(price)"],"metadata":{"id":"pEg679UWA0DB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prods['specs'] = specs # create a new column in dataframe that contains specification"],"metadata":{"id":"3JVchd-yGqAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prods.to_csv(f'{path}specs_carisinyal.csv',index=False) # save the dataframe into a csv file"],"metadata":{"id":"V00EkEAvpIAo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing carisinyal scrapped data"],"metadata":{"id":"_lA_X6ABfprL"}},{"cell_type":"markdown","source":["24 April 2024"],"metadata":{"id":"cm7Y68oHs3mZ"}},{"cell_type":"code","source":["# create path to the files\n","path = '/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - Smartphone/ScrappedRAW_Data/'"],"metadata":{"id":"Q0i6TwzBf4Vp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = pd.read_csv(f'{path}specs_carisinyal.csv') # read the files to create dataframe"],"metadata":{"id":"hGBuE_h8pmLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this to revert the string into dictionary because after saved it csv dictionary type will turn into string\n","# df1['specification'] = df1['specification'].apply(lambda x : ast.literal_eval(x))\n","\n","# create new columns by splitting specification column\n","# df1['key'] = df1['specification'].apply(lambda x : x['key'])\n","# df1['val'] = df1['specification'].apply(lambda x : x['val'])"],"metadata":{"id":"kvpvmIPitlTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this to revert the string into dictionary because after saved it csv dictionary type will turn into string\n","# df1['key'] = df1['key'].apply(lambda x : ast.literal_eval(x))\n","# df1['val'] = df1['val'].apply(lambda x : ast.literal_eval(x))"],"metadata":{"id":"MEdcbhXbyLzq","executionInfo":{"status":"ok","timestamp":1713935305222,"user_tz":-420,"elapsed":10,"user":{"displayName":"Alfian Khofi","userId":"00807384603454567370"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1dcf3cb9-a365-4cc4-f7d0-a3d6b3f47df0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-8b19014c5722>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df1['key'] = df1['key'].apply(lambda x : ast.literal_eval(x))\n","<ipython-input-11-8b19014c5722>:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df1['val'] = df1['val'].apply(lambda x : ast.literal_eval(x))\n"]}]},{"cell_type":"code","source":["val_df = pd.DataFrame(df1.val.tolist(), index= df1.index) # this to split the data inside lists in val column into multiple columns\n","key_df = pd.DataFrame(df1.key.tolist(), index= df1.index) # this to split the data inside lists in key column into multiple columns"],"metadata":{"id":"6Ylw_g9zxKeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this to create unique identifier\n","val_df = val_df.reset_index().rename(columns={'index':'id'})\n","key_df = key_df.reset_index().rename(columns={'index':'id'})\n","df1 = df1.reset_index().rename(columns={'index':'id'})"],"metadata":{"id":"jiD7yKKByEPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge the data altoghether or like perform SQL Join on id and products column\n","k_df = df1[['id','products']].merge(key_df,how='left')\n","v_df = df1[['id','products']].merge(val_df,how='left')"],"metadata":{"id":"rLtKGL_26olc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# concat the two dataframes above into one on row axis\n","df = pd.concat([k_df,v_df],axis=0)"],"metadata":{"id":"5BHVTJHahnKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.sort_values('id') # sort the data by id"],"metadata":{"id":"e9-cQBW3hzfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(f'{path}carisinyal.csv',index=False) # save it into csv file"],"metadata":{"id":"HRMwxRKY6vPD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scrape Reviews"],"metadata":{"id":"V4Nbg-h1nWbS"}},{"cell_type":"markdown","source":["21 April 2024"],"metadata":{"id":"EUusjVBTv7I8"}},{"cell_type":"code","source":["# create variable for target url\n","urls = ['https://www.gsmarena.com/infinix_hot_40_pro-reviews-12733.php']"],"metadata":{"id":"H4XCxPmNnVPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# looping for pagination since i don't use selenium\n","for i in range(2,9):\n","  urls.append(f'https://www.gsmarena.com/infinix_hot_40_pro-reviews-12733p{i}.php')"],"metadata":{"id":"qL9nXFTG9n6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = [] # empty variable to store the value of for loop\n","for url in urls: # loop through the target url\n","  r = requests.get(url) # make a request to url and the value obtained is called response\n","  res = r.content # here is the response content\n","  parser = BeautifulSoup(res,'html.parser') # parsed into html using BeautifulSoup\n","  comments = parser.find_all('p',attrs={'class':'uopin'}) # using find_all method to find the tag and class containing comment text\n","  for comment in comments: # for loop to loop through comments tag\n","    reviews.append(comment.text) # insert only the text from comments value to empty list that has been created earlier"],"metadata":{"id":"WBdoRYF7nqC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(reviews) # check the number of comments scrapped"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn7cRmAzocS5","executionInfo":{"status":"ok","timestamp":1713708323572,"user_tz":-420,"elapsed":511,"user":{"displayName":"Alfian Khofi","userId":"00807384603454567370"}},"outputId":"c16fe863-65ca-43dc-8a53-b05c938a4e14"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["149"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["dfr = pd.DataFrame(reviews,columns=['reviews']) # create dataframe to make it easier for cleaning process later"],"metadata":{"id":"u4fNlO_Yodu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define functions for cleaning the text comments\n","def clean_username(text):\n","  if text.find('2024') < 0: # this take the text and if not find the '2024' text it will return the original text\n","    return text\n","  else:  # and this if the text is containing word '2024' it will skip couple of words containing username and date\n","    return text[text.find('2024')+4:]\n","def clean_replies(text):\n","  if text.find('...') < 0: # this to find unfinished sentences same as before if it doesn't find '...' it will return the original text\n","    return text\n","  else:\n","    return text[text.find('...')+8:] # and if it finds '...' it will skips the sentence before the '...'"],"metadata":{"id":"jwQHxWviqKB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfr['reviews'] = dfr['reviews'].apply(clean_username) # apply the first function\n","dfr['reviews'] = dfr['reviews'].apply(clean_replies) # apply the second function\n","dfr['reviews'] = dfr['reviews'].str.lower() # make all the text to be lowercase\n","dfr['reviews'] = dfr['reviews'].str.replace('\\n','') # delete all newlines for each comment or review\n","dfr['reviews'] = dfr['reviews'].str.strip() # delete whitespaces in the begining or ending of each comment or review\n","dfr['reviews'] = dfr['reviews'].str.replace('.','').str.replace(\"'\",'') # delete all the period and single quote\n","dfr['reviews'] = dfr['reviews'].replace(r'[^\\w]',' ',regex=True) # delete all the symbols\n","dfr['reviews'] = dfr['reviews'].str.replace('  ',' ') # replace the double spaces with single space"],"metadata":{"id":"WqCmGXUt0bU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the file as csv\n","dfr.to_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - Smartphone/ScrappedRAW_Data/reviews.csv',index=False)"],"metadata":{"id":"DzuIa4OQAAss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Additional Scrape Reviews from Websites"],"metadata":{"id":"J9QC1d4l8-gs"}},{"cell_type":"markdown","source":["21 April 2024"],"metadata":{"id":"3aOOj5T9wBEk"}},{"cell_type":"markdown","source":["Review articles from a couple of websites"],"metadata":{"id":"dCdFxNBG35-X"}},{"cell_type":"code","source":["nopyre = requests.get('https://www.noypigeeks.com/android/infinix-hot-40-pro-review/')\n","con = nopyre.content\n","html = BeautifulSoup(con,'html.parser')\n","full_review = html.find_all('p')"],"metadata":{"id":"lTHm4qvY1bPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nopiyreviews = [x.text for x in full_review]\n","n_reviews = nopiyreviews[2:-13]\n","len(n_reviews)"],"metadata":{"id":"8V_T7CVH2EUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rev_91 = requests.get('https://www.91mobiles.com/hub/infinix-hot-30i-review/')\n","resp = rev_91.content\n","html_parser = BeautifulSoup(resp,'html.parser')\n","full_91_review = html_parser.find_all('p')"],"metadata":{"id":"DbdRvIbx3ZFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews_91 = [x.text for x in full_91_review]\n","web_review = n_reviews + reviews_91"],"metadata":{"id":"Ans6pwcB3q8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfrw = pd.DataFrame(web_review,columns=['reviews'])"],"metadata":{"id":"Hh9dsTfx4uKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfrw['reviews'] = dfrw['reviews'].str.lower() # make all the text to be lowercase\n","dfrw['reviews'] = dfrw['reviews'].str.replace('\\n','') # delete all newlines for each comment or review\n","dfrw['reviews'] = dfrw['reviews'].str.strip() # delete whitespaces in the begining or ending of each comment or review\n","dfrw['reviews'] = dfrw['reviews'].str.replace('.','').str.replace(\"'\",'') # delete all the period and single quote\n","dfrw['reviews'] = dfrw['reviews'].replace(r'[^\\w]',' ',regex=True) # delete all the symbols\n","dfrw['reviews'] = dfrw['reviews'].str.replace('  ',' ') # replace the double spaces with single space"],"metadata":{"id":"lcuk10VL5AmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = pd.concat([dfr,dfrw],axis=0)\n","reviews = reviews.reset_index(drop=True)\n","\n","reviews.to_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - Smartphone/ScrappedRAW_Data/Reviews.csv',index=False)"],"metadata":{"id":"pZMl0sKu5kZ-"},"execution_count":null,"outputs":[]}]}